{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to 'redacted_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Output file for processed data\n",
    "output_file = 'redacted_file.csv'\n",
    "# Columns to keep in the new DataFrame\n",
    "columns_to_keep = ['Date', 'HomeTeam', 'AwayTeam', 'B365H', 'B365D', 'B365A', 'LBH', 'LBD', 'LBA', 'FTR', 'FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC']\n",
    "\n",
    "# Folder path for CSV files\n",
    "data_folder_path = r'C:\\Users\\marku\\Documents\\GitHub\\Odds\\Odds\\data'\n",
    "\n",
    "\n",
    "def process_csv_files(data_folder_path, output_file, columns_to_keep):\n",
    "\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(columns_to_keep)  # Write column names at the top\n",
    "        \n",
    "        for filename in os.listdir(data_folder_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                file_path = os.path.join(data_folder_path, filename)\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8', errors='replace') as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file)\n",
    "                    header = next(csv_reader)  # Get the header\n",
    "                    \n",
    "                    # Identify the indices of the columns to keep\n",
    "                    column_indices = [header.index(col) if col in header else None for col in columns_to_keep]\n",
    "                    \n",
    "                    for row in csv_reader:\n",
    "                        # Handle missing values and replace invalid characters\n",
    "                        for idx, value in enumerate(row):\n",
    "                            if value == 'ï¿½':\n",
    "                                row[idx] = ''\n",
    "  \n",
    "                        # Select only the desired columns using identified indices\n",
    "                        selected_row = [row[idx] if idx is not None else '' for idx in column_indices]\n",
    "                        \n",
    "                        # Handle missing values\n",
    "                        for col_bet365, col_ladbrokes in zip(['B365H', 'B365D', 'B365A'], ['LBH', 'LBD', 'LBA']):\n",
    "                            idx_bet365 = columns_to_keep.index(col_bet365)\n",
    "                            idx_ladbrokes = columns_to_keep.index(col_ladbrokes)\n",
    "                            \n",
    "                            if selected_row[idx_bet365] == '':\n",
    "                                selected_row[idx_bet365] = selected_row[idx_ladbrokes]\n",
    "                       \n",
    "                        # Append the selected row to the output file\n",
    "                        writer.writerow(selected_row)\n",
    "\n",
    "    print(f\"Processed data saved to '{output_file}'.\")\n",
    "\n",
    "# Call the function to process CSV files\n",
    "process_csv_files(data_folder_path, output_file, columns_to_keep)\n",
    "\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "def process_csv_files2(df):\n",
    "    # Remove LBH, LBD and LBA columns (not needed)\n",
    "    df = df.drop(['LBH', 'LBD', 'LBA'], axis=1)\n",
    "\n",
    "    # Create a column showing shot accuracy (shots on target / total shots, as percentage)\n",
    "    df['HomeShotsAcc'] = (df['HST'] / df['HS']).astype(float).map(\"{:.2%}\".format)\n",
    "\n",
    "    # Shot accuracy for away team (as percentage)\n",
    "    df['AwayShotsAcc'] = (df['AST'] / df['AS']).astype(float).map(\"{:.2%}\".format)\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = process_csv_files2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute simple aggregates on the odds, goal difference, goal totals, shots, and shots on target for each team in the Premier League\n",
    "def stats1 (df):\n",
    "    print(df[['B365H']].describe())\n",
    "    print('')\n",
    "    print(df[['B365D']].describe())\n",
    "    print('')\n",
    "    print(df[['B365A']].describe())\n",
    "    print('')\n",
    "    return df\n",
    "    \n",
    "#stats1(df)\n",
    "\n",
    "# Compute stats for each team in the Premier League\n",
    "def stats2(df):\n",
    "    # Group by 'HomeTeam' and calculate statistics\n",
    "    home_team_stats = df.groupby('HomeTeam')[['B365H', 'B365D', 'B365A']].describe()\n",
    "    \n",
    "    # Group by 'AwayTeam' and calculate statistics\n",
    "    away_team_stats = df.groupby('AwayTeam')[['B365H', 'B365D', 'B365A']].describe()\n",
    "    \n",
    "    # Return the stats\n",
    "    df = home_team_stats, away_team_stats\n",
    "    return df\n",
    "\n",
    "#stats2(df)\n",
    "\n",
    "# Compute rolling averages on performance metrics for each team in the Premier League over the past 5 matches\n",
    "def stats3(df):\n",
    "    # Compute rolling averages for home team performance metrics (shots, shots on target, goals, goal difference)\n",
    "    df['HomeShotsRolling'] = df.groupby('HomeTeam', group_keys=False)['HS'].apply(lambda x: x.rolling(center=False, window=5).mean())\n",
    "    df['HomeShotsOnTargetRolling'] = df.groupby('HomeTeam', group_keys=False)['HST'].apply(lambda x: x.rolling(center=False, window=5).mean())\n",
    "    df['HomeGoalsRolling'] = df.groupby('HomeTeam', group_keys=False)['FTHG'].apply(lambda x: x.rolling(center=False, window=5).mean())\n",
    "    \n",
    "    # Compute rolling averages for away team performance metrics (shots, shots on target, goals, goal difference)\n",
    "    df['AwayShotsRolling'] = df.groupby('AwayTeam', group_keys=False)['AS'].apply(lambda x: x.rolling(center=False, window=5).mean())\n",
    "    df['AwayShotsOnTargetRolling'] = df.groupby('AwayTeam', group_keys=False)['AST'].apply(lambda x: x.rolling(center=False, window=5).mean())\n",
    "    df['AwayGoalsRolling'] = df.groupby('AwayTeam', group_keys=False)['FTAG'].apply(lambda x: x.rolling(center=False, window=5).mean())\n",
    "    \n",
    "    # Drop missing values from Rolling columns\n",
    "    df = df.dropna(subset=['HomeShotsRolling', 'HomeShotsOnTargetRolling', 'HomeGoalsRolling', 'AwayShotsRolling', 'AwayShotsOnTargetRolling', 'AwayGoalsRolling'])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = stats3(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Mapping:\n",
      "Code: 0, Team: Arsenal\n",
      "Code: 1, Team: Aston Villa\n",
      "Code: 2, Team: Birmingham\n",
      "Code: 3, Team: Blackburn\n",
      "Code: 4, Team: Blackpool\n",
      "Code: 5, Team: Bolton\n",
      "Code: 6, Team: Bournemouth\n",
      "Code: 7, Team: Brentford\n",
      "Code: 8, Team: Brighton\n",
      "Code: 9, Team: Burnley\n",
      "Code: 10, Team: Cardiff\n",
      "Code: 11, Team: Charlton\n",
      "Code: 12, Team: Chelsea\n",
      "Code: 13, Team: Crystal Palace\n",
      "Code: 14, Team: Derby\n",
      "Code: 15, Team: Everton\n",
      "Code: 16, Team: Fulham\n",
      "Code: 17, Team: Huddersfield\n",
      "Code: 18, Team: Hull\n",
      "Code: 19, Team: Leeds\n",
      "Code: 20, Team: Leicester\n",
      "Code: 21, Team: Liverpool\n",
      "Code: 22, Team: Man City\n",
      "Code: 23, Team: Man United\n",
      "Code: 24, Team: Middlesbrough\n",
      "Code: 25, Team: Newcastle\n",
      "Code: 26, Team: Norwich\n",
      "Code: 27, Team: Nott'm Forest\n",
      "Code: 28, Team: Portsmouth\n",
      "Code: 29, Team: QPR\n",
      "Code: 30, Team: Reading\n",
      "Code: 31, Team: Sheffield United\n",
      "Code: 32, Team: Southampton\n",
      "Code: 33, Team: Stoke\n",
      "Code: 34, Team: Sunderland\n",
      "Code: 35, Team: Swansea\n",
      "Code: 36, Team: Tottenham\n",
      "Code: 37, Team: Watford\n",
      "Code: 38, Team: West Brom\n",
      "Code: 39, Team: West Ham\n",
      "Code: 40, Team: Wigan\n",
      "Code: 41, Team: Wolves\n",
      "FTR encodings:  [2. 0. 1.]\n",
      "FTR mappings:  ['H' 'A' 'D']\n"
     ]
    }
   ],
   "source": [
    "def format_dtypes(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    min_date = df['Date'].min()\n",
    "    df['Date'] = (df['Date'] - min_date).dt.days\n",
    "    df['Date'] = df['Date'].astype('float64')\n",
    "    \n",
    "    # Convert HomeTeam and AwayTeam to category codes\n",
    "    df['HomeTeamCode'] = df['HomeTeam'].astype('category').cat.codes\n",
    "    df['AwayTeamCode'] = df['AwayTeam'].astype('category').cat.codes\n",
    "    team_mapping = dict(enumerate(df['HomeTeam'].astype('category').cat.categories))\n",
    "    \n",
    "    # Convert FTR to category codes\n",
    "    df['FTR'] = df['FTR'].astype('category').cat.codes\n",
    "\n",
    "    # Convert numeric columns\n",
    "    numerical_columns = ['B365H', 'B365D', 'B365A', 'FTHG', 'FTAG',\n",
    "                         'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HomeShotsAcc',\n",
    "                         'AwayShotsAcc', 'HomeShotsRolling', 'HomeShotsOnTargetRolling', 'HomeGoalsRolling',\n",
    "                         'AwayShotsRolling', 'AwayShotsOnTargetRolling',\n",
    "                         'AwayGoalsRolling']\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Handle columns with percentage signs\n",
    "            if 'Acc' in col or 'Rate' in col:\n",
    "                df[col] = df[col].str.replace('%', '').astype('float').fillna(0) / 100.0\n",
    "            else:\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                except ValueError:\n",
    "                    print(f\"Error converting column: {col}\")\n",
    "                    pass\n",
    "        else:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            except ValueError:\n",
    "                print(f\"Error converting column: {col}\")\n",
    "                pass\n",
    "        \n",
    "    # Convert HomeTeamCode and AwayTeamCode to float64 and FTR to float64\n",
    "    df['HomeTeamCode'] = df['HomeTeamCode'].astype('float64')\n",
    "    df['AwayTeamCode'] = df['AwayTeamCode'].astype('float64')\n",
    "    df['FTR'] = df['FTR'].astype('float64')\n",
    "\n",
    "    # Remove HomeTeam and AwayTeam columns\n",
    "    df = df.drop(['HomeTeam', 'AwayTeam'], axis=1)\n",
    "    \n",
    "    return df, team_mapping\n",
    "\n",
    "# Call the format_dtypes function to get the modified DataFrame and team_mapping dictionary\n",
    "df, team_mapping = format_dtypes(df)\n",
    "\n",
    "df.to_csv('test.csv', index=False)\n",
    "\n",
    "# Print the team_mapping dictionary\n",
    "print(\"Team Mapping:\")\n",
    "for code, team in team_mapping.items():\n",
    "    print(f\"Code: {code}, Team: {team}\")\n",
    "\n",
    "# Print FTR encodings\n",
    "print(\"FTR encodings: \", df.FTR.unique())\n",
    "\n",
    "# Print their mappings to original values\n",
    "print(\"FTR mappings: \", df.FTR.map({0: 'A', 1: 'D', 2: 'H'}).unique())\n",
    "\n",
    "# Save as a csv file called team_mapping.csv\n",
    "team_mapping_df = pd.DataFrame.from_dict(team_mapping, orient='index', columns=['Team'])\n",
    "team_mapping_df.index.name = 'Code'\n",
    "team_mapping_df.reset_index(inplace=True)\n",
    "team_mapping_df.to_csv('team_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    # Exclude columns that don't make sense for correlation matrix\n",
    "    excluded_columns = ['HomeTeamCode', 'AwayTeamCode']\n",
    "    \n",
    "    # Select only numeric columns for correlation matrix\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    valid_columns = [col for col in numeric_columns if col not in excluded_columns]\n",
    "    correlation_matrix = df[valid_columns].corr()\n",
    "    \n",
    "    # Plot the correlation heatmap\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a list of coefficient pairs and sort in descending order\n",
    "    correlations = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "    \n",
    "    # Filter out pairs where both variables are the same or have a correlation of 1\n",
    "    filtered_correlations = correlations[\n",
    "        (correlations.index.get_level_values(0) != correlations.index.get_level_values(1)) &\n",
    "        (correlations != 1.0)\n",
    "    ]\n",
    "    \n",
    "    # Print the sorted list of coefficient pairs\n",
    "    print(\"Top Correlations:\")\n",
    "    print(filtered_correlations)\n",
    "\n",
    "# correlation_matrix(df)\n",
    "\n",
    "def pairplot2(df):\n",
    "    sns.set(style=\"white\")\n",
    "    sns.pairplot(df, diag_kind=\"kde\")\n",
    "    plt.show()\n",
    "\n",
    "# pairplot2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "    # Features\n",
    "    X = df.drop(columns=['FTR', 'FTHG', 'FTAG'])\n",
    "\n",
    "    # Target variable (label 1: Home Win, label 0: Draw or Away Win)\n",
    "    y = df['FTR'].apply(lambda x: 1.0 if x == 2.0 else 0.0)\n",
    "\n",
    "    # Splitting the data into train, test and validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = data_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Random Forest Classifier with 100 estimators and feature importance analysis\"\"\"\n",
    "    # Initialize Random Forest Classifier\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the results\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "RandomForest(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = RandomForest(X_train, X_val, y_train, y_val)\n",
    "feature_importance = pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVM(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Linear SVM classifier\"\"\"\n",
    "    # Create a classifier\n",
    "    svm = LinearSVC()\n",
    "\n",
    "    # Train the classifier\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the outcome of a game\n",
    "    prediction = svm.predict(X_val)\n",
    "\n",
    "    # Print the accuracy score\n",
    "    print(accuracy_score(y_val, prediction))\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(y_val, prediction))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_val, prediction)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "LinearSVM(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_classifier(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"SVC classifier with linear kernel and C=0.01\"\"\"\n",
    "    # Create SVM classifier\n",
    "    svm_clf = SVC()\n",
    "\n",
    "    # Fit classifier to training set\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict FTR for test set\n",
    "    y_pred = svm_clf.predict(X_val)\n",
    "\n",
    "    # Evaluate performance of classifier\n",
    "    print('Accuracy:', accuracy_score(y_val, y_pred))\n",
    "\n",
    "    # classification report\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "SVC_classifier(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GB_classifier(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Gradient Boosting Classifier with log loss function, 20000 estimators, learning rate of 0.1, and max depth of 1\"\"\"\n",
    "    # Create a Gradient Boosting Classifier\n",
    "    GB = GradientBoostingClassifier()\n",
    "\n",
    "    # Fit the model\n",
    "    GB.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the model\n",
    "    y_pred = GB.predict(X_val)\n",
    "\n",
    "    # Print the accuracy score\n",
    "    print(\"Accuracy score:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report: \\n {}\\n\".format(classification_report(y_val, y_pred)))\n",
    "    \n",
    "    return GB\n",
    "\n",
    "GB_classifier(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Logistic Regression with 1000 iterations\"\"\"\n",
    "    # Create a logistic regression classifier\n",
    "    log_reg = LogisticRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the model\n",
    "    y_pred = log_reg.predict(X_val)\n",
    "\n",
    "    # Print the accuracy score\n",
    "    print(\"Accuracy score:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report: \\n {}\\n\".format(classification_report(y_val, y_pred)))\n",
    "\n",
    "logistic_regression(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "300 fits failed out of a total of 900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.74320388 0.74676375 0.74676375 0.74627832\n",
      "        nan        nan 0.74320388 0.74676375 0.74644013 0.74627832\n",
      "        nan        nan 0.74320388 0.74676375 0.74644013 0.74627832\n",
      "        nan        nan 0.74320388 0.74676375 0.74644013 0.74627832\n",
      "        nan        nan 0.74320388 0.74676375 0.74644013 0.74627832\n",
      "        nan        nan 0.74320388 0.74676375 0.74644013 0.74627832\n",
      "        nan        nan 0.75469256 0.75339806 0.75339806 0.75355987\n",
      "        nan        nan 0.75469256 0.75339806 0.75307443 0.75355987\n",
      "        nan        nan 0.75469256 0.75339806 0.75307443 0.75355987\n",
      "        nan        nan 0.75469256 0.75339806 0.75307443 0.75355987\n",
      "        nan        nan 0.75469256 0.75339806 0.75307443 0.75355987\n",
      "        nan        nan 0.75469256 0.75339806 0.75307443 0.75355987\n",
      "        nan        nan 0.75275081 0.75275081 0.75550162 0.75372168\n",
      "        nan        nan 0.75275081 0.75275081 0.75469256 0.75372168\n",
      "        nan        nan 0.75275081 0.75275081 0.75469256 0.75372168\n",
      "        nan        nan 0.75275081 0.75275081 0.75469256 0.75372168\n",
      "        nan        nan 0.75275081 0.75275081 0.75469256 0.75372168\n",
      "        nan        nan 0.75275081 0.75275081 0.75469256 0.75372168\n",
      "        nan        nan 0.75210356 0.75161812 0.75469256 0.75161812\n",
      "        nan        nan 0.75210356 0.75161812 0.75582524 0.75161812\n",
      "        nan        nan 0.75210356 0.75161812 0.75582524 0.75161812\n",
      "        nan        nan 0.75210356 0.75161812 0.75582524 0.75161812\n",
      "        nan        nan 0.75210356 0.75161812 0.75582524 0.75161812\n",
      "        nan        nan 0.75210356 0.75161812 0.75582524 0.75161812\n",
      "        nan        nan 0.75161812 0.7512945  0.75453074 0.75080906\n",
      "        nan        nan 0.75161812 0.7512945  0.75323625 0.75080906\n",
      "        nan        nan 0.75161812 0.7512945  0.75323625 0.75080906\n",
      "        nan        nan 0.75161812 0.7512945  0.75323625 0.75080906\n",
      "        nan        nan 0.75161812 0.7512945  0.75323625 0.75080906\n",
      "        nan        nan 0.75161812 0.7512945  0.75323625 0.75080906]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'max_iter': 2000, 'penalty': 'l2', 'random_state': 0, 'solver': 'lbfgs'}\n",
      "Best score: 0.7558252427184465\n",
      "\n",
      "Model: RandomForestClassifier\n",
      "Best parameters: {'criterion': 'gini', 'max_depth': None, 'n_estimators': 200, 'random_state': 0}\n",
      "Best score: 0.737378640776699\n",
      "\n",
      "Model: SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=50000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1000, 'kernel': 'rbf', 'max_iter': 50000, 'random_state': 0}\n",
      "Best score: 0.693042071197411\n",
      "\n",
      "Model: LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "245 fits failed out of a total of 490.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "245 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.59983819        nan 0.60339806        nan 0.61666667\n",
      "        nan 0.66359223        nan 0.69546926        nan 0.72799353\n",
      "        nan 0.73236246        nan 0.6276699         nan 0.55857605\n",
      "        nan 0.61779935        nan 0.67346278        nan 0.66650485\n",
      "        nan 0.67022654        nan 0.72734628        nan 0.64012945\n",
      "        nan 0.61909385        nan 0.57443366        nan 0.59093851\n",
      "        nan 0.60889968        nan 0.65339806        nan 0.65064725\n",
      "        nan 0.66569579        nan 0.60048544        nan 0.64530744\n",
      "        nan 0.63802589        nan 0.49708738        nan 0.60857605\n",
      "        nan 0.59886731        nan 0.63074434        nan 0.62912621\n",
      "        nan 0.64773463        nan 0.62491909        nan 0.61682848\n",
      "        nan 0.60453074        nan 0.60469256        nan 0.63511327\n",
      "        nan 0.60776699        nan 0.59805825        nan 0.59886731\n",
      "        nan 0.61877023        nan 0.58754045        nan 0.61666667\n",
      "        nan 0.63495146        nan 0.59822006        nan 0.54708738\n",
      "        nan 0.58122977        nan 0.64045307        nan 0.54919094\n",
      "        nan 0.60048544]\n",
      "  warnings.warn(\n",
      "c:\\Users\\marku\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.001, 'max_iter': 100000, 'penalty': 'l2', 'random_state': 0}\n",
      "Best score: 0.7323624595469255\n",
      "\n",
      "Model: GradientBoostingClassifier\n",
      "Best parameters: {'learning_rate': 0.1, 'loss': 'log_loss', 'max_features': 'sqrt', 'n_estimators': 200, 'random_state': 0}\n",
      "Best score: 0.7529126213592232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'random_state': [0],\n",
    "    'n_estimators': [100, 200, 500, 1000, 2000],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "    'criterion': ['gini', 'log_loss']\n",
    "}\n",
    "\n",
    "param_grid_linSVM = {\n",
    "    'random_state': [0],\n",
    "    'max_iter': [1000, 2000, 5000, 10000, 20000, 50000, 100000],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "param_grid_SVC = {\n",
    "    'random_state': [0],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [1000, 2000, 5000, 10000, 20000, 50000]\n",
    "}\n",
    "\n",
    "param_grid_GB = {\n",
    "    'random_state': [0],\n",
    "    'n_estimators': [100, 200, 500, 1000, 2000],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100],\n",
    "    'loss': ['log_loss'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "param_grid_log_reg = {\n",
    "    'random_state': [0],\n",
    "    'max_iter': [1000, 2000, 5000, 10000, 20000, 50000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Combine all parameter grids into a dictionary\n",
    "param_grids = {\n",
    "    RandomForestClassifier: param_grid_rf,\n",
    "    LinearSVC: param_grid_linSVM,\n",
    "    SVC: param_grid_SVC,\n",
    "    GradientBoostingClassifier: param_grid_GB,\n",
    "    LogisticRegression: param_grid_log_reg\n",
    "}\n",
    "\n",
    "# Dictionary of model functions\n",
    "models = {\n",
    "    RandomForestClassifier,\n",
    "    LinearSVC,\n",
    "    SVC,\n",
    "    GradientBoostingClassifier,\n",
    "    LogisticRegression\n",
    "}\n",
    "\n",
    "def grid_search(models, param_grids, X_train, y_train):\n",
    "    \"\"\"GridSearchCV to find the best parameters and models\"\"\"\n",
    "    for model_class in models:\n",
    "        model_name = model_class.__name__\n",
    "        print(f\"Model: {model_name}\")\n",
    "        param_grid = param_grids[model_class]\n",
    "        model = model_class()  # Instantiate the model class\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best score: {grid_search.best_score_}\")\n",
    "        print('')\n",
    "\n",
    "grid_search(models, param_grids, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
